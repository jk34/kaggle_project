{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.features import create_train_features\n",
    "from exp.run import run_experiment\n",
    "from exp.mappings import alg_map\n",
    "from exp.train import train_model\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_save = \"X_tr.csv\"\n",
    "y_save = \"y_tr.csv\"\n",
    "X_save_scaled = \"X_tr_scaled.csv\"\n",
    "scale_params_pickle = \"scale_params.pickle\"\n",
    "other_params_json = \"other.json\"\n",
    "tr_scaler = None\n",
    "classic_sta_lta5_mean_fill = None\n",
    "classic_sta_lta7_mean_fill = None\n",
    "\n",
    "if not (os.path.exists(X_save_scaled) and os.path.exists(y_save)):\n",
    "    if os.path.exists(X_save) and os.path.exists(y_save):\n",
    "        X_tr = pd.read_csv(X_save, index_col=0)\n",
    "        y_tr = pd.read_csv(y_save, index_col=0)\n",
    "\n",
    "        scale_params_pickle_on = open(scale_params_pickle, \"rb\")\n",
    "        tr_scaler = pickle.load(scale_params_pickle_on)\n",
    "        scale_params_pickle_on.close()\n",
    "        \n",
    "        X_train_scaled = pd.DataFrame(tr_scaler.transform(X_tr), columns=X_tr.columns)\n",
    "        X_train_scaled.to_csv(X_save_scaled)\n",
    "    else:\n",
    "        X_tr, X_train_scaled, y_tr, tr_scaler, classic_sta_lta5_mean_fill, classic_sta_lta7_mean_fill  = create_train_features(r'C:\\Users\\arvin\\dev\\lanl\\train.csv')\n",
    "        X_tr.to_csv(X_save)\n",
    "        y_tr.to_csv(y_save)\n",
    "        X_train_scaled.to_csv(X_save_scaled)\n",
    "\n",
    "        scale_params_pickle_on = open(scale_params_pickle, \"wb\")\n",
    "        pickle.dump(tr_scaler, scale_params_pickle_on)\n",
    "        scale_params_pickle_on.close()\n",
    "\n",
    "        with open(other_params_json, 'w') as fp:\n",
    "            json.dump({\"classic_sta_lta5_mean_fill\": classic_sta_lta5_mean_fill,\n",
    "                       \"classic_sta_lta7_mean_fill\": classic_sta_lta7_mean_fill}, fp)\n",
    "else:\n",
    "    X_train_scaled = pd.read_csv(X_save_scaled, index_col=0)\n",
    "    y_tr = pd.read_csv(y_save, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       mean       std       max       min  mean_change_abs  mean_change_rate  \\\n",
      "0  1.424140 -0.170214 -0.218194  0.193218        -1.326420         -1.569265   \n",
      "1  0.805716  0.004734  0.063936 -0.018037         0.002747         -1.040206   \n",
      "2  1.511155  0.049252 -0.086289  0.163039        -0.218781          0.949925   \n",
      "3  1.494934  0.043950  0.122560 -0.187796         0.002747         -0.634909   \n",
      "4  1.520242  0.088495 -0.067969  0.087590        -0.108017          0.094279   \n",
      "\n",
      "    abs_max  abs_min  std_first_50000  std_last_50000  ...  \\\n",
      "0 -0.222567      0.0         0.052067       -0.285579  ...   \n",
      "1  0.036797      0.0         0.153858       -0.076987  ...   \n",
      "2 -0.101306      0.0         0.004241        0.277885  ...   \n",
      "3  0.097427      0.0         0.020852       -0.058805  ...   \n",
      "4 -0.084464      0.0        -0.093104        0.174161  ...   \n",
      "\n",
      "   std_roll_mean_1000  max_roll_mean_1000  min_roll_mean_1000  \\\n",
      "0            0.268470           -0.004742            0.178278   \n",
      "1           -0.141264            0.007341           -0.025387   \n",
      "2            0.085078            0.099556            0.245184   \n",
      "3            0.083085            0.068076            0.105059   \n",
      "4           -0.164151            0.138032            0.187535   \n",
      "\n",
      "   q01_roll_mean_1000  q05_roll_mean_1000  q95_roll_mean_1000  \\\n",
      "0            0.287332            0.965402            1.509153   \n",
      "1            0.622391            0.842747            0.522428   \n",
      "2            0.634878            1.207106            1.530919   \n",
      "3            0.770151            1.160208            1.432972   \n",
      "4            1.040695            1.557034            1.393068   \n",
      "\n",
      "   q99_roll_mean_1000  av_change_abs_roll_mean_1000  \\\n",
      "0            0.885262                     -0.631300   \n",
      "1            0.294357                     -0.912054   \n",
      "2            0.889790                      0.441128   \n",
      "3            0.815078                     -0.949994   \n",
      "4            0.901110                      0.595416   \n",
      "\n",
      "   av_change_rate_roll_mean_1000  abs_max_roll_mean_1000  \n",
      "0                      -1.832422               -0.004742  \n",
      "1                      -0.890022                0.007341  \n",
      "2                       0.639209                0.099556  \n",
      "3                      -1.097513                0.068076  \n",
      "4                      -0.465464                0.138032  \n",
      "\n",
      "[5 rows x 138 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyper-parameter experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example of Cartesian Product of Hyper-parameters for Linear Regression\n",
    "\n",
    "\"lr\": {\"fit_intercept\": [False, True], \"normalize\": [False, True]}\n",
    "\n",
    "Cartesian Product: {fit_intercept} x {normalize}\n",
    "\n",
    "Hyper-parameter choices:\n",
    "\"fit_intercept\": False, \"normalize\": False\n",
    "\"fit_intercept\": True, \"normalize\": False\n",
    "\"fit_intercept\": False, \"normalize\": True\n",
    "\"fit_intercept\": True, \"normalize\": True\n",
    "\"\"\"\n",
    "\n",
    "params={\"lr\": {\"fit_intercept\": [False, True], \"normalize\": [False, True]},\n",
    "       \"ridge\": {\"alpha\": [.000001, .00001, .0001, .001, .01, .1, 1.0, 10, 100],\n",
    "                 \"fit_intercept\": [False, True], \"normalize\": [False, True]},\n",
    "       \"lasso\": {\"alpha\": [.000001, .00001, .0001, .001, .01, .1, 1.0, 10, 100],\n",
    "                 \"fit_intercept\": [False, True], \"normalize\": [False, True],\n",
    "                 \"positive\": [False, False, False, False, False, True],\n",
    "                 \"selection\": [\"cyclic\", \"cyclic\", \"cyclic\", \"cyclic\", \"cyclic\", \"random\"]},\n",
    "       \"mtlasso\": {\"alpha\": [.000001, .00001, .0001, .001, .01, .1, 1.0, 10, 100],\n",
    "                   \"fit_intercept\": [False, True], \"normalize\": [False, True],\n",
    "                   \"selection\": [\"cyclic\", \"cyclic\", \"cyclic\", \"cyclic\", \"cyclic\", \"random\"]},\n",
    "       \"elastic\": {\"alpha\": [.000001, .00001, .0001, .001, .01, .1, 1.0, 10, 100],\n",
    "                   \"fit_intercept\": [False, True], \"normalize\": [False, True], \n",
    "                   \"positive\": [False, False, False, False, False, True],\n",
    "                   \"l1_ratio\": [.01, .99, .2, .4, .6, .8], \n",
    "                   \"selection\": [\"cyclic\", \"cyclic\", \"cyclic\", \"cyclic\", \"cyclic\", \"random\"]},\n",
    "       \"lars\": {\"fit_intercept\": [False, True], \"normalize\": [False, True],\n",
    "                \"fit_path\": [False], \"n_nonzero_coefs\": [10, 100, 500, 1000, 10000, np.inf]},\n",
    "       \"llars\": {\"alpha\": [.000001, .00001, .0001, .001, .01, .1, 1.0, 10, 100],\n",
    "                 \"fit_intercept\": [False, True], \"normalize\": [False, True],\n",
    "                \"fit_path\": [False], \"positive\": [False, False, False, False, False, True]},\n",
    "       \"omp\": {\"fit_intercept\": [False, True], \"normalize\": [False, True],\n",
    "               \"n_nonzero_coefs\": [10, 100, None, None, None]},\n",
    "       \"sgdreg\": {\"loss\": [\"squared_loss\", \"squared_loss\", \"squared_loss\", \"huber\", \"epsilon_insensitive\",\n",
    "                           \"squared_epsilon_insensitive\"],\n",
    "                  \"penalty\": [\"none\", \"l2\", \"l1\", \"elasticnet\"], \n",
    "                  \"alpha\": [.000001, .00001, .0001, .001, .01, .1, 1.0, 10, 100],\n",
    "                  \"l1_ratio\": [.01, .99, .2, .4, .6, .8], \"fit_intercept\": [False, True],\n",
    "                  \"learning_rate\": [\"constant\", \"optimal\", \"optimal\", \"optimal\", \"invscaling\", \"adaptive\"],\n",
    "                  \"eta0\": [1.0, 10.0, .1, .01, .001, .0001],\n",
    "                  \"early_stopping\": [False, False, False, False, True]},\n",
    "       \"pareg\": {\"C\": [.001, .01, .1, 1.0, 1.0, 1.0, 10.0, 100.0],\n",
    "                 \"loss\": [\"epsilon_insensitive\", \"squared_epsilon_insensitive\"],\n",
    "                 \"epsilon\": [.01, .05, .1, .1, .1, .5],\n",
    "                 \"early_stopping\": [False, False, False, False, True]},\n",
    "        # \"tsreg\": {\"fit_intercept\": [False, True]},\n",
    "        \"hreg\": {\"epsilon\": [1.1, 1.2, 1.35, 1.35, 1.35, 1.35, 1.5, 1.6, 1.8, 2.0, 2.5],\n",
    "                 \"alpha\": [.000001, .00001, .0001, .001, .01, .1, 1.0, 10, 100],\n",
    "                 \"fit_intercept\": [False, True]},\n",
    "        \"kreg\": {\"alpha\": [.000001, .00001, .0001, .001, .01, .1, 1.0, 10, 100],\n",
    "                 \"kernel\": [\"linear\", \"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "                 \"gamma\": [None, None, None, None, .001, .0001, .01, .1]}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled shape:  (4194, 138)\n",
      "4194\n",
      "PCA explained variance ratio (first 50 components): [4.67527491e-01 1.31896039e-01 8.03803322e-02 6.13857166e-02\n",
      " 2.83457425e-02 2.18411289e-02 1.89761955e-02 1.74868597e-02\n",
      " 1.61917596e-02 1.32777708e-02 1.24577647e-02 1.01964819e-02\n",
      " 8.92283770e-03 7.74412458e-03 7.51586380e-03 7.06741373e-03\n",
      " 6.64359994e-03 6.21740502e-03 5.73372613e-03 5.72723554e-03\n",
      " 5.08934278e-03 4.68587039e-03 4.52714989e-03 4.22541710e-03\n",
      " 3.97234346e-03 3.25515367e-03 3.07030383e-03 2.87927045e-03\n",
      " 2.57634585e-03 2.40247232e-03 2.30163654e-03 2.21519737e-03\n",
      " 2.04076390e-03 1.88802447e-03 1.68950822e-03 1.62597775e-03\n",
      " 1.53856217e-03 1.28517069e-03 1.12919067e-03 1.09043990e-03\n",
      " 9.21865509e-04 9.08570581e-04 8.58039676e-04 8.15359349e-04\n",
      " 7.10127140e-04 6.64709283e-04 5.57369136e-04 5.42748514e-04\n",
      " 4.54130642e-04 4.06653623e-04]\n",
      "cumulative variance PCA\n",
      "[46.8 60.  68.  74.1 76.9 79.1 81.  82.7 84.3 85.6 86.8 87.8 88.7 89.5\n",
      " 90.3 91.  91.7 92.3 92.9 93.5 94.  94.5 95.  95.4 95.8 96.1 96.4 96.7\n",
      " 97.  97.2 97.4 97.6 97.8 98.  98.2 98.4 98.6 98.7 98.8 98.9 99.  99.1\n",
      " 99.2 99.3 99.4 99.5 99.6 99.7 99.7 99.7]\n",
      "cumulative variance PCA - 40\n",
      "[46.8 60.  68.  74.1 76.9 79.1 81.  82.7 84.3 85.6 86.8 87.8 88.7 89.5\n",
      " 90.3 91.  91.7 92.3 92.9 93.5 94.  94.5 95.  95.4 95.8 96.1 96.4 96.7\n",
      " 97.  97.2 97.4 97.6 97.8 98.  98.2 98.4 98.6 98.7 98.8 98.9]\n",
      "\n",
      "X_train_scaled.shape:  (4194, 138)\n",
      "(PCA) X_r.shape:  (4194, 40)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train_scaled shape: \", X_train_scaled.shape)\n",
    "\n",
    "#need to use numpy array for PCA\n",
    "Ymat = y_tr.as_matrix()\n",
    "Xmat = X_train_scaled.as_matrix()\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=50)\n",
    "X_r = pca.fit(Xmat).transform(Xmat)\n",
    "#print(\"X_r\", X_r[1:5])\n",
    "print(len(X_r))\n",
    "\n",
    "print('PCA explained variance ratio (first 50 components): %s'\n",
    "      % str(pca.explained_variance_ratio_))\n",
    "\n",
    "print(\"cumulative variance PCA\")\n",
    "variance = pca.explained_variance_ratio_ #calculate variance ratios\n",
    "\n",
    "var=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3)*100)\n",
    "print(var) #cumulative sum of variance explained with [n] features\n",
    "\n",
    "\n",
    "pca = PCA(n_components=40)\n",
    "X_r = pca.fit(Xmat).transform(Xmat)\n",
    "print(\"cumulative variance PCA - 40\")\n",
    "variance = pca.explained_variance_ratio_ #calculate variance ratios\n",
    "var=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3)*100)\n",
    "print(var) #cumulative sum of variance explained with [n] features\n",
    "\n",
    "print(\"\\nX_train_scaled.shape: \",X_train_scaled.shape)\n",
    "print(\"(PCA) X_r.shape: \",X_r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first 40 principal components explains 98.9% of variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA on acoustic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7a91e4007a7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'acoustic_data'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'time_to_failure'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nrows'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1993\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers._concatenate_chunks\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6aadbd3d21fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'acoustic_data'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'time_to_failure'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#need to use numpy array for PCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mYmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mXmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "#need to use numpy array for PCA\n",
    "Ymat = y_tr.as_matrix()\n",
    "Xmat = train.as_matrix()\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=50)\n",
    "X_r = pca.fit(Xmat).transform(Xmat)\n",
    "#print(\"X_r\", X_r[1:5])\n",
    "print(len(X_r))\n",
    "\n",
    "print('PCA explained variance ratio (first 50 components): %s'\n",
    "      % str(pca.explained_variance_ratio_))\n",
    "\n",
    "print(\"cumulative variance PCA\")\n",
    "variance = pca.explained_variance_ratio_ #calculate variance ratios\n",
    "\n",
    "var=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3)*100)\n",
    "print(var) #cumulative sum of variance explained with [n] features\n",
    "\n",
    "\n",
    "pca = PCA(n_components=40)\n",
    "X_r = pca.fit(Xmat).transform(Xmat)\n",
    "print(\"cumulative variance PCA - 40\")\n",
    "variance = pca.explained_variance_ratio_ #calculate variance ratios\n",
    "var=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3)*100)\n",
    "print(var) #cumulative sum of variance explained with [n] features\n",
    "\n",
    "print(\"\\nX_train_scaled.shape: \",X_train_scaled.shape)\n",
    "print(\"(PCA) X_r.shape: \",X_r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stats' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-264a7dbfd4d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#can also normalize the data with the Box-Cox transformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# transform training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mX_train_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxcox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stats' is not defined"
     ]
    }
   ],
   "source": [
    "#StandardScaler normalizes the features so that each feature has mean=0, std=1\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(X_train_scaled)\n",
    "y_tr = sc.fit_transform(y_tr)\n",
    "\n",
    "\n",
    "from scipy.stats import boxcox\n",
    "#can also normalize the data with the Box-Cox transformation\n",
    "# transform training set\n",
    "X_train_scaled = stats.boxcox(X_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlapping segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-93f6a432c285>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#obtained from https://www.kaggle.com/alinealmeida/basic-feature-benchmark-with-quantiles-augmenting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'acoustic_data'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'time_to_failure'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#obtained from https://www.kaggle.com/alinealmeida/basic-feature-benchmark-with-quantiles-augmenting\n",
    "train = pd.read_csv('train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})\n",
    "\n",
    "print(train.head())\n",
    "rows=150000\n",
    "#stride of 75000\n",
    "shift_step = int(np.floor(rows / 2))\n",
    "segments = int(np.floor(train.shape[0] / rows))\n",
    "segments_augmented = 2*segments - 1\n",
    "\n",
    "X_train = pd.DataFrame(index=range(segments_augmented), dtype=np.float64,\n",
    "                       columns=['ave', 'std', 'max', 'min','q95','q99', 'q05','q01'])\n",
    "y_train = pd.DataFrame(index=range(segments_augmented), dtype=np.float64,\n",
    "                       columns=['time_to_failure'])\n",
    "\n",
    "for segment in tqdm(range(segments)):\n",
    "    for do_shift in [False,True]:        \n",
    "        if(do_shift):\n",
    "            shift = shift_step\n",
    "            idx = segments + segment            \n",
    "            if(segment==segments-1): #last segment would be incomplete for the shifted version\n",
    "                continue\n",
    "        else:\n",
    "            shift = 0\n",
    "            idx = segment\n",
    "        \n",
    "        seg = train.iloc[segment*rows+shift:segment*rows+shift+rows]\n",
    "\n",
    "        x = seg['acoustic_data'].values\n",
    "        y = seg['time_to_failure'].values[-1]\n",
    "\n",
    "        y_train.loc[idx, 'time_to_failure'] = y\n",
    "\n",
    "        X_train.loc[idx, 'ave'] = x.mean()\n",
    "        X_train.loc[idx, 'std'] = x.std()\n",
    "        X_train.loc[idx, 'max'] = x.max()\n",
    "        X_train.loc[idx, 'min'] = x.min()\n",
    "        X_train.loc[idx, 'q95'] = np.quantile(x,0.95)\n",
    "        X_train.loc[idx, 'q99'] = np.quantile(x,0.99)\n",
    "        X_train.loc[idx, 'q05'] = np.quantile(x,0.05)\n",
    "        X_train.loc[idx, 'q01'] = np.quantile(x,0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_searches=20\n",
    "n_fold=10\n",
    "save_results= \"exp.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr\n",
      "ridge\n",
      "lasso\n",
      "mtlasso\n",
      "elastic\n",
      "lars\n",
      "llars\n",
      "omp\n",
      "sgdreg\n",
      "pareg\n",
      "hreg\n",
      "kreg\n"
     ]
    }
   ],
   "source": [
    "for alg in params.keys():\n",
    "    print(alg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alg in params.keys():\n",
    "    print(alg)\n",
    "    score_df = run_experiment(X=X_train_scaled, Y=y_tr, n_fold=n_fold, alg=alg, alg_params=params[alg], search_type=\"random\", num_searches=num_searches, save_results=save_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display models ranked by CV scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = score_df.sort_values(by=\"score\", axis=0)\n",
    "display(score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load results from CSV File and re-produce models ranked by CV scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.read_csv(save_results)\n",
    "score_df = score_df.sort_values(by=\"score\", axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best model from CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve top scoring row\n",
    "best = score_df.iloc[0]\n",
    "display(best)\n",
    "\n",
    "# retrieve model parameters from pandas row\n",
    "alg = best[\"alg\"]\n",
    "params_json = best[\"params_json\"]\n",
    "print(\"alg: {}\".format(alg))\n",
    "print(\"params_json: {}\".format(params_json))\n",
    "\n",
    "# retrieve relevant values\n",
    "alg_cls = alg_map[alg]\n",
    "params = json.loads(params_json)\n",
    "\n",
    "# initialize model\n",
    "model = alg_cls(**params)\n",
    "\n",
    "# train algorithm\n",
    "train_model(X=X_train_scaled, Y=y_tr, n_fold=n_fold, model=model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
